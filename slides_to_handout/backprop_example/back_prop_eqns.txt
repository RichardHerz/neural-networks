Below, instead of standard math notation, we use MATLAB pseudo-code in order to make comparison of this derivation to the MATLAB m-file easier. 
        
A derivative below similar to dX_dY is the rate of change of X with respect to change in Y. We do not use dX/dY since that shows division in code.
        
Relationships in the XOR network: 
        
        layer           input      hidden        output
        
        activation      a{1}         a{2}          a{3} to approx y  
        weight                W{1}         W{2} 
        deltas                       d{1}          d{2}
        delta W               dW{1}        dW{2}
        
        The errors at the output nodes are 
        
          E = 0.5 * (y - a{3}).^2 
          
          dE_da{3} = -(y - a{3})
          
        Define the inputs from layer 2 going to the output layer as 
        
          I{2} = W{2} * a{2} 
        
        where 
        
          a{3} = sigmaFunc(I2) 
          
          da{3}_dI{2} = sigmaFunc(I2)_dI{2} 
          
                    = a{3} .* (1 - a{3}) 
          
        The rates of change of Error with respect to I{2}, the signals from layer 2 to layer 3, are 
        
          dE_dI{2} = dE_da{3} * da{3}_dI{2} 
          
                   = -(y - a{3}) .* a{3} .* (1 - a{3}) 
                   
        Name these derivatives d{2} 
        
          d{2} = dE_dI{2} = -(y - a{3}) .* a{3} .* (1 - a{3}) 
              
        Working backward, in "backward propagation" of the Error... 
        
          I{1} = W{1} * a{1} 
          
>>>>  CHECK matrix multiplication here and if need inverse 
OK with no inverse, since from code: 
            % without biases B
            % a{i} = sigmaFunc( W{i-1}*a{i-1} );
        
          a{2} = sigmaFunc(I1) 
          
          da{2}_dI{1} = sigmaFunc(I1)_dI{1} 
          
                    = a{2} .* (1 - a{2}) 
                    
          dE_da{2} = dI{2}_da{2} * dE_dI{2} 
                
          dI{2}_da{2} = W{2} 
        
          dE_dI{2} = d{2} 
        
          dE_da{2} = W{2}' * d{2}   
          
        where W{2}' is the transpose of W{2} and * is matrix multiplication in Matlab such that the results are the sums of the contributions to each node in layer 2. Combining, we get... 
          
        The rates of change of Error with respect to I{1}, the signals from layer 1 to layer 2, are
       
          dE_dI{1} = dE_da{2} * da{2}_I{1} 
        
          dE_dI{1} = W{2}' * d{2} .* a{2} .* (1 - a{2}) 
          
        Name these derivatives d{1} 
        
          d{1} = dE_dI{1} = W{2}' * d{2} .* a{2} .* (1 - a{2}) 
          
        For the XOR network with only one hidden layer, we stop here in getting the d's. For a network with more hidden layers, we would continue the process by which we got d{1}, with a change in array index, of course. 
        
        Now that we have the d's, the rates of change of Error with respect to the inputs to each layer, we can compute the rates of change of Error with respect to the weights themselves. 
        
          dI{i}_dW{i} = a{i}
          
          dE_dW{i} = dE_dI{i} * dI{i}_dW{i}
          
          dE_dW{i} = d{i} * a{i}'
          
          where a{i}' is the transpose of a{i} and * is matrix multiplication in Matlab such that the results are the sums of the contributions to each node in layer i. 
          
<<< CHECK TO EACH NODE ABOVE (to each weight...?) >>>
          
        Name these derivatives dW{i} 
          
          dW{i} = dE_dW{i} = d{i} * a{i}' 
          
        Now we can compute the new values of the connection weights W{i}, which will be used in the next iteration of the gradient descent method 
          
          W{i} = W{i} - alpha * dW{i}
          
        where alpha is a constant value in the range 0-1. 
        

        