AS OF APRIL 24, 2020, 11:30 am - only XOR_neural.m has been updated 

ONLY CODE FROM HERE TO END TRAINING CODE SECTION 

% initialize node activations
ai = zeros(numInputNodes,batchsize);
ah = zeros(numHiddenNodes,batchsize); 
ao = zeros(numOutputNodes,batchsize);
a = {ai};
for j = 2:numHiddenLayers+1
    a{j} = ah;
end
a{numHiddenLayers + 2} = ao;

% initialize connection weights randomly in range -1 to +1
wi = 2*rand(numHiddenNodes,numInputNodes) - 1;
wh = 2*rand(numHiddenNodes,numHiddenNodes) - 1;
wo = 2*rand(numOutputNodes,numHiddenNodes) - 1;

if (numHiddenLayers > 1)
    W = {wi};
    for j = 2:numHiddenLayers
        W{j} = wh;
    end
    W{numHiddenLayers + 1} = wo;
elseif (numHiddenLayers == 1)
    W{1} = wi;
    W{2} = wo;
else
    fprintf(' xxxx this program requires at least 1 hidden layer xxxxx \n')
    return
end

% initialize biases
initBias = 0;
Bh = initBias * ones(numHiddenNodes,1);
Bo = zeros(numOutputNodes,1);
for j = 1:numHiddenLayers
    B{j} = Bh;
end
B{numHiddenLayers+1} = Bo;
dB = B;

for j = 1 : numepochs
    % randomly rearrange the training data for each epoch
    % We keep the shuffled index in kk, so that the input and output could
    % be matched together
    kk = randperm(size(train_x, 2)); 

    for b = 1 : numbatches 

        a{1} = train_x(:, kk( (b-1)*batchsize+1 : b*batchsize ) ); 
        y = train_y(:, kk( (b-1)*batchsize+1 : b*batchsize ) );
        
        %{
        Note: when two or more matrices are put into one cell of a
        cell array, they are combined into a single matrix in that cell;
        e.g., 4 examples of 2 input arrays in one cell creates a 2x4 array
        in that cell
        %}
               
        % Forward propagation
        
        for j = 2 : numHiddenLayers + 2
            % without biases B
            % a{i} = sigmaFunc( W{i-1}*a{i-1} );
            % with biases B
            a{j} = sigmaFunc(bsxfun( @plus, W{j-1}*a{j-1}, B{j-1} ) );
        end
              
        %{
        Start Back-Propagation in order to train network and
        minimize error by adjusting weights using gradient
        descent method
         
        Relationships in this XOR network: 

        layer           input      hidden        output
        activation      a{1}         a{2}          a{3} to approx y  
        weight                W{1}         W{2} 

        Error
        
          j = numHiddenLayers+1
          E = 0.5 * ( y - a{j+1} ).^2
        
        Derivative of error with respect to output
        
          dE_da{j+1} = - ( y - a{numHiddenLayers+2} ) 
        
        Define inputs I from last hidden layer going to the output layer
     
          I{j} = W{j} * a{j}
        
          a{j+1} = sigmaFunc( I{j} )
          da{j+1}_dI{j} = dsigmaFunc(I{j})_dI{j}
                        = a{j+1} .* (1 - a{j+1})
        
          dE_dI{j} = dE_da{j+1} * da{j+1}_dI{j}
                   = -(y - a{j+1}) * a{j+1} .* (1 - a{j+1})
        %} 
        
        j = numHiddenLayers+1;
        dE_dI{j} = -(y - a{j+1}) .* a{j+1} .* (1 - a{j+1});

        %{
        moving "back" toward the input, 
        
        for j = numHiddenLayers : -1 : 1
        
          I{j} = W{j} * a{j}
          a{j+1} = sigmaFunc(I{j})
          da{j+1}_dI{j} = dsigmaFunc(I{j})_dI{j}
                        = a{j+1} .* (1 - a{j+1})
        
          dE_da{j+1} = dI{j+1}_da{j+1} * dE_I{j+1}
          dI{j+1}_da{j+1} = W{j+1} 
          dE_da{j+1} = W{j+1}' * dE_I{j+1}
                
          dE_dI{j} = dE_da{j+1} * da{j+1}_dI{j}
                   = W{j+1}' * dE_I{j+1} * a{j+1} .* (1 - a{j+1})
        end
        %}
        
        for j = numHiddenLayers : -1 : 1
            dE_dI{j} = W{j+1}' * dE_dI{j+1} .* a{j+1} .* (1 - a{j+1});
        end 

        %{
        Update connection weights using the gradient descent method
        
        for j = 1 : numHiddenLayers+1
        
          dE_dW{j} = dE_dI{j} * dI{j}_dW{j}
          dI{j}_dW{j} = a{j}
        
          dE_dW{j} = dE_dI{j} * a{j}' << the gradients of error w/r weights
        
          W{j} = W{j} - alpha * dE_dW{j}  << without L2 regularization
 
        end
        %}

        for j = 1 : numHiddenLayers+1
            dE_dW{j} = dE_dI{j} * a{j}';
            % L2 regularization is used for W, which is the lambda * W term 
            W{j} = W{j} - alpha * (dE_dW{j} - lambda * W{j}); 
        end

%         % update biases added to nodes in hidden layers
%         for i = 1 : numHiddenLayers
%             dB{i} = sum(d{i},2);
%             B{i} = B{i} + alpha * dB{i};
%         end
        
    end
end

% save workspace so we can use below after clearing 
save('WS')
