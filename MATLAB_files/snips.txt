% neural network example
% based on pp. 29=32 of Chap5.3-BackProp.pdf by Sargur Srihari 
% lesson 5.3 of https://cedar.buffalo.edu/~srihari/CSE574/ 
% with modifications 

% EXAMPLE OF XOR from  https://www.mladdict.com/neural-network-simulator 

% FOR GRADIENT DESCENT METHOD used in BACK PROPAGATION see 
% https://en.wikipedia.org/wiki/Gradient_descent 

% >>>> THERE ARE SEVERAL CODE SECTIONS BELOW <<<<<<<



if (numHiddenLayers > 1)
    W = {wi};
    for j = 2:numHiddenLayers
        W{j} = wh;
    end
    W{numHiddenLayers + 1} = wo;
elseif (numHiddenLayers == 1)
    W{1} = wi;
    W{2} = wo;
else
    fprintf(' xxxx this program requires at least 1 hidden layer xxxxx \n')
    return
end 

    ------------------------------------------------------------ 

   %{
    start back-propagation 

    The total error to be minimized is sum(0.5*(y - a).^2) at the output
    layer
    
    The gradient descent algorithm is used to update the connection 
    weights between each pair of layers in order to minimize 
    the total error at the final output nodes. This is done in the
    "back" direction from the output layer back toward the input layer.

    For the last, output layer, the error derivatives d{numHuddenLayers+1} 
    are the derivatives of the total error with respect to the estimated 
    outputs multiplied by the derivatives of the estimated outputs 
    with respect to the arguments of the sigma function (the a*(1-a) terms)
    %}
    i = numHiddenLayers+1;
    d{i} = -(y - a{i+1}) .* a{i+1} .* (1 - a{i+1});

    %{
    moving "back" toward the input,
    for all preceding nodes i, numHuddenLayers down to 1, the error 
    delta d is the later d at i+1, multiplied by the weights and
    then by the derivatives of the estimated outputs 
    with respect to the arguments of the sigma function (the a*(1-a) terms)
    %}
    for i = numHiddenLayers : -1 : 1
        d{i} = W{i+1}' * d{i+1} .* a{i+1} .* (1 - a{i+1});
    end 

    % update weights after all error delta d's have been computed
    for i = 1 : numHiddenLayers+1
        dW{i} = d{i} * a{i}';
        % use the gradient descent method, where
        % L2 regularization is used for W, which is the lambda * W term 
        W{i} = W{i} - alpha * (dW{i} - lambda * W{i}); 
    end

------------------------------------------------------------ 

%% Testing

% just loading same workspace but is useful
% for re-using the weights W
% when only running this or following sections

------------------------------------------------------------  

    %% set an single input to image below 

tn = 2; % uses tn several places below, which single input to use  


    